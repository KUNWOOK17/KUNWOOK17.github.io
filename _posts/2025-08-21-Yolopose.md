---
title: "AI 기반 협동 로봇 작업 보조 시스템: YOLO 객체 탐지 및 인간-로봇 상호작용(HRI) 지능을 활용한 비전 기반 접근법"
excerpt: "YOLO 기반 인공지능 비전 기술을 활용한 협동 로봇 작업 보조 시스템"
date: 2025-08-21
layout: post
categories: [Precision Control, ROS2, Doosan M0609, STT, TTS, Blob]
tags: [YOLOv8, Doosan M0609, STT, Blob, TTS]
toc: true
toc_sticky: true
---

> *"기계가 보고 이해하기 시작할 때, 협업은 비로소 직관이 된다."*

---
<div class="youtube-wrapper">
  <iframe
    src="https://www.youtube.com/embed/Fas-aIrPaJc"
    title="Liquid Injection Control Demo"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
  </iframe>
</div>


---
## Abstract

본 프로젝트는 협동 로봇인 **두산 M0609**와 Realsense, Yolo 를 활용하여, **가정용 실시간 반응 로봇** 개발을 다룹니다. 주어진 **두산 M0609** 모델과 Realsense의 기능적 활용을 최대화하기 위해 **AI 기반 협동 로봇 작업 보조 시스템**을 구현했습니다.

---

## 1. 서론 및 문제 정의

![Robotics market comparison](/assets/images/Yolopose/Robotics_market_comparison.png)  
*그림 1. 로봇 산업 주요 세부 시장의 연도별 시장 규모 성장 추세 (USD 기준)*

위 공개 수치를 바탕으로, 가정 내 로봇팔은 아직 “전체 가정용 로봇 시장 속 작은 하위영역”이지만
① 가정용 로봇 시장 자체의 고성장, ② 로봇팔 가격 하락과 소형화, ③ AI 기반 조작(비전·LLM)의 성능 향상, ④ 고령화로 인한 재가 돌봄/보조 조작 수요 증가에 힘입어 중기적으로 빠르게 확대될 가능성이 크다

---

## 2. 시스템 아키텍처 및 설계
![node Overview](/assets/images/Yolopose/node_system.png)  
*그림 2: Realsense와 M0609 협업 및 통신 구조를 포함한 전체 시스템 아키텍처*

![System Overview](/assets/images/Yolopose/system_overview.png)  
*그림 3: 시스템의 전체 흐름 및 예시*

### 2.1 기술 스택 개요

| 구성 요소 | 구현 기술 | 선택 근거 |
|----------|----------|-----------|
| **객체 탐지** | YOLOv11 | 속도와 정확도 간 최적의 균형 |
| **자세 추정** | YOLOv11-pose | 실시간 키포인트 기반 자세 추정으로 동작/방향을 안정적으로 파악 |
| **좌표 변환** | TF2 프레임워크 | 카메라–로봇 간 좌표계를 실시간으로 변환하는 ROS2 네이티브 프레임워크 |
| **기술 구현** | 비전–로봇 통합 파이프라인 | 인식·좌표 변환·로봇 제어를 하나의 실시간 처리 흐름으로 통합 |
| **플랫폼** | Ubuntu 22.04 + ROS2 | 안정성 및 활발한 커뮤니티 지원 |


---

## 3. 객체 탐지 시스템

### 3.1 데이터셋 준비 및 모델 선정

**데이터셋 세부 사양:**
- **총 샘플 수:** 약 4,000개 (훈련: 2,800개, 검증: 800개, 테스트: 400개)
- **클래스:** phone, book, wallet, laptop, hand, vitamin, airpods, pencilcase
- **형식:** YOLO 어노테이션 형식
- **해상도:** 640×640 픽셀 (학습 시 리사이즈)
- **데이터 증강:** 회전, 스케일 변화, 밝기 변화를 포함한 기본적인 이미지 증강 적용


### 3.2 모델 성능 분석

추론 시간 분포를 이용한 YOLO 모델 간 비교 분석 결과는 다음과 같습니다:

![System Overview](/assets/images/Yolopose/Yolo_comparechart.png)  
*그림 4: Yolo 모델 간 비교 분석 결과*


| 모델           | 추론 시간 (ms/img) | COCO mAP@0.5:0.95 | 특징                     |
| :----------- | :------------: | :---------------: | :--------------------- |
| YOLOv5n      |      ~4.0      |       ~0.38       | 경량 모델, 비교 기준선          |
| YOLOv8n      |      ~3.5      |       ~0.39       | v5 대비 정확도·속도 개선        |
| YOLOv9n      |      ~3.0      |       ~0.38       | 연산 효율 중심 설계            |
| **YOLOv11n** |    **~2.0**    |     **~0.40**     | **최소 지연 시간 대비 최고 정확도** |
| YOLOv11s     |      ~2.5      |       ~0.47       | 소형 모델 중 높은 정확도         |
| YOLOv11m     |      ~5.0      |       ~0.52       | 정확도–속도 균형              |
| YOLOv11x     |      ~12.0     |       ~0.55       | 최고 성능, 높은 연산 비용        |


**선정 근거:** YOLOv11n은 추론시간이 짧아 빠른 응답속도와 최고 수준 정확도를 보여줍니다.

---

## 4. Yolov11n

### 4.1 Pose estimation
### 4.2 지원되는 작업 및 모드

YOLO11은 이전 Ultralytics YOLO 릴리스에서 구축한 다목적 모델 범위를 기반으로, 다양한 컴퓨터 비전 작업에 대해 향상된 지원을 제공한다.

| 모델 | 파일 이름 | 작업 | 추론 | 검증 | 훈련 | 내보내기 |
| :--- | :--- | :--- | :---: | :---: | :---: | :---: |
| YOLO11 | yolov11n.pt<br>yolov11s.pt<br>yolov11m.pt<br>yolov11l.pt<br>yolov11x.pt | 객체 탐지 | ✅ | ✅ | ✅ | ✅ |
| YOLO11-seg | yolov11n-seg.pt<br>yolov11s-seg.pt<br>yolov11m-seg.pt<br>yolov11l-seg.pt<br>yolov11x-seg.pt | 인스턴스 분할 | ✅ | ✅ | ✅ | ✅ |
| YOLO11-pose | yolov11n-pose.pt<br>yolov11s-pose.pt<br>yolov11m-pose.pt<br>yolov11l-pose.pt<br>yolov11x-pose.pt | 포즈/키포인트 | ✅ | ✅ | ✅ | ✅ |
| YOLO11-obb | yolov11n-obb.pt<br>yolov11s-obb.pt<br>yolov11m-obb.pt<br>yolov11l-obb.pt<br>yolov11x-obb.pt | 방향 감지 (OBB) | ✅ | ✅ | ✅ | ✅ |
| YOLO11-cls | yolov11n-cls.pt<br>yolov11s-cls.pt<br>yolov11m-cls.pt<br>yolov11l-cls.pt<br>yolov11x-cls.pt | 분류 | ✅ | ✅ | ✅ | ✅ |

이 표는 YOLO11 모델의 다양한 변형에 대한 개요를 제공하며, 특정 작업에서의 적용 가능성과 추론, 검증, 훈련 및 내보내기와 같은 작동 모드와의 호환성을 보여준다. 이러한 유연성 덕분에 YOLO11은 실시간 감지부터 복잡한 분할 작업에 이르기까지 컴퓨터 비전 분야의 광범위한 응용에 적합하다.

**YOLOv11 주요 특징 및 장점**

- **향상된 특징 추출**  
  YOLOv11은 개선된 백본(backbone) 및 넥(neck) 아키텍처를 사용하여 보다 정확한 객체 탐지와 복잡한 작업 수행을 위한 특징 추출 성능을 향상시킨다.

- **효율성 및 속도에 최적화된 설계**  
  개선된 아키텍처 설계와 최적화된 학습 파이프라인을 도입하여 더 빠른 처리 속도를 제공하며, 정확도와 성능 간의 최적 균형을 유지한다.

- **더 적은 매개변수로 더 높은 정확도**  
  모델 설계의 발전을 통해 YOLOv11m은 YOLOv8m 대비 약 22% 적은 매개변수를 사용하면서도 COCO 데이터셋에서 더 높은 mean Average Precision(mAP)을 달성하여 계산 효율성을 향상시켰다.

- **다양한 환경에 대한 높은 적응성**  
  YOLOv11은 엣지 디바이스, 클라우드 플랫폼, NVIDIA GPU 기반 시스템 등 다양한 환경에 원활하게 배포될 수 있도록 설계되어 높은 유연성을 제공한다.

- **광범위한 비전 작업 지원**  
  객체 탐지(Object Detection), 인스턴스 분할(Instance Segmentation), 이미지 분류(Image Classification), 포즈 추정(Pose Estimation), OBB(Oriented Bounding Box Detection) 등 다양한 컴퓨터 비전 작업을 단일 프레임워크에서 지원한다.

### 4.3 Pose estimation

### 4.4 성능 (Pose · COCO)

아래 표는 COCO 데이터셋에서 학습된 YOLOv11 Pose 모델들의 성능을 비교한 결과이다.  
모든 모델은 입력 해상도 640×640 기준으로 평가되었으며, 정확도(mAP)와 추론 속도, 모델 복잡도를 함께 제시한다.

| 모델 | 입력 크기 (픽셀) | mAP<sub>pose</sub> 50–95 | mAP<sub>pose</sub> 50 | 속도 CPU ONNX (ms) | 속도 T4 TensorRT10 (ms) | 파라미터 (M) | FLOPs (B) |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| YOLOv11n-pose | 640 | 50.0 | 81.0 | 52.4 ± 0.5 | 1.7 ± 0.0 | 2.9 | 7.4 |
| YOLOv11s-pose | 640 | 58.9 | 86.3 | 90.5 ± 0.6 | 2.6 ± 0.0 | 9.9 | 23.1 |
| YOLOv11m-pose | 640 | 64.9 | 89.4 | 187.3 ± 0.8 | 4.9 ± 0.1 | 20.9 | 71.4 |
| YOLOv11l-pose | 640 | 66.1 | 89.9 | 247.7 ± 1.1 | 6.4 ± 0.1 | 26.1 | 90.3 |
| YOLOv11x-pose | 640 | 69.5 | 91.1 | 488.0 ± 13.9 | 12.1 ± 0.2 | 58.8 | 202.8 |


---

## 5. STT & TTS

### 5.1 STT

STT는 음성 입력을 Mel Spectrogram으로 변환한 후, , Transformer 기반 인코더-디코더 구조를 통해 문장 단위 텍스트를 예측한다.
본 프로젝트에서 사용한 Whisper 모델은 다국어·잡음 환경에서도 강건하며, 음성 전사뿐 아니라 번역·언어 식별까지 지원한다.

### 5.2 Mel Spectrogram

**신호 (Signals)**

신호란 시간에 따라 어떤 물리량이 변화하는 현상을 의미한다.  
오디오 신호의 경우, 시간에 따라 변화하는 물리량은 **공기 압력(air pressure)**이다.

이러한 정보를 디지털 형태로 기록하기 위해, 우리는 시간에 따른 공기 압력을 **샘플링(sampling)**하여 측정한다.  
샘플링 속도는 응용 분야에 따라 달라질 수 있으나, 가장 일반적으로 사용되는 값은 **44.1kHz**, 즉 **초당 44,100개의 샘플**이다.

이 과정을 통해 얻어진 결과는 해당 신호의 **파형(waveform)**이며,  
이 파형은 컴퓨터 소프트웨어를 통해 시각화되거나 해석, 수정 및 분석될 수 있다.

이제 우리는 오디오 신호를 디지털 형태로 표현했으며, 이를 자유롭게 다룰 수 있게 되었다.  
신호 처리(signal processing) 분야에 오신 것을 환영한다!

하지만 한 가지 의문이 생길 수 있다.  
이 복잡해 보이는 파형에서 **어떻게 유용한 정보를 추출할 수 있을까?**  
시간 영역에서의 신호는 종종 뒤섞인 형태로 보이며, 직관적으로 해석하기 어렵다.  
바로 이 지점에서 우리의 친구 **푸리에(Fourier)**가 등장한다.

**푸리에 변환 (Fourier Transform)**

오디오 신호는 여러 개의 **단일 주파수(sound wave)** 성분이 결합된 형태로 구성되어 있다.  
시간에 따라 신호를 샘플링할 때, 우리는 이 성분들이 합쳐진 결과인 **진폭(amplitude)**만을 관측하게 된다.

**푸리에 변환(Fourier Transform)**은 이러한 신호를 개별 주파수 성분과 각 주파수의 진폭으로 분해할 수 있도록 해주는 수학적 변환이다.  
즉, 신호를 **시간 영역(time domain)**에서 **주파수 영역(frequency domain)**으로 변환하는 역할을 한다.

이 변환의 결과는 **스펙트럼(spectrum)**이라 불리며,  
신호에 어떤 주파수 성분이 포함되어 있는지를 명확하게 보여준다.

### 5.3 Spectrogram

고속 푸리에 변환(FFT)은 신호에 포함된 **주파수 성분을 분석**할 수 있는 강력한 도구이다.  
하지만 만약 **신호의 주파수 성분이 시간에 따라 변한다면** 어떻게 해야 할까?

음악이나 음성과 같은 대부분의 오디오 신호가 바로 이런 경우에 해당한다.  
이러한 신호들은 **비주기 신호(non-periodic signals)** 라고 불린다.

우리는 이러한 신호들의 **시간에 따른 스펙트럼 변화를 표현할 수 있는 방법**이 필요하다.

여기서 이런 생각이 들 수도 있다.

> “잠깐, 신호를 여러 개의 작은 구간(window)으로 나눈 다음, 각각에 FFT를 적용하면  
> 여러 개의 스펙트럼을 얻을 수 있지 않을까?”

맞다! 바로 이 방법이 사용되며, 이를 **단시간 푸리에 변환(STFT, Short-Time Fourier Transform)** 이라고 한다.

STFT에서는 신호를 **서로 겹치는(windowed) 구간들로 나눈 뒤**, 각 구간마다 FFT를 계산한다.  
이렇게 해서 얻어지는 결과가 바로 **스펙트로그램(spectrogram)** 이다.

와! 한 번에 받아들이기엔 정말 많은 내용이다.  
여기에는 다양한 개념들이 한꺼번에 얽혀 있다.  
그래서 이 개념을 이해하기 위해서는 **시각적인 그림이 큰 도움이 된다.**





```python
import librosa
import librosa.display
import matplotlib.pyplot as plt
y, sr = librosa.load('./example_data/88b3a759.wav')
plt.plot(y);
plt.title('Signal');
plt.xlabel('Time (samples)');
plt.ylabel('Amplitude');
```
![System Overview](/assets/images/Yolopose/Mel_Spectrogram_1.png)  
*그림 5: 시간 영역에서의 오디오 신호 파형*


```python
import numpy as np
n_fft = 2048
ft = np.abs(librosa.stft(y[:n_fft], hop_length = n_fft+1))
plt.plot(ft);
plt.title('Spectrum');
plt.xlabel('Frequency Bin');
plt.ylabel('Amplitude');
```
![System Overview](/assets/images/Yolopose/Mel_Spectrogram_2.png)  
*그림 6: 푸리에 변환을 통해 얻은 오디오 신호의 주파수 스펙트럼*



```python
spec = np.abs(librosa.stft(y, hop_length=512))
spec = librosa.amplitude_to_db(spec, ref=np.max)
librosa.display.specshow(spec, sr=sr, x_axis='time', y_axis='log');
plt.colorbar(format='%+2.0f dB');
plt.title('Spectrogram');
```
![System Overview](/assets/images/Yolopose/Mel_Spectrogram_3.png)  
*그림 7: 단시간 푸리에 변환(STFT)을 이용해 계산한 오디오 신호의 스펙트로그램*


![System Overview](/assets/images/Yolopose/Mel_Spectrogram_5.png)  
*그림 8: 단시간 푸리에 변환(STFT)을 이용한 스펙트로그램 생성 과정*

스펙트로그램은 **여러 개의 FFT를 위로 차곡차곡 쌓아 올린 것**이라고 생각할 수 있다.  
이는 서로 다른 주파수에서 신호의 **크기(음량 또는 진폭)** 가  
**시간에 따라 어떻게 변하는지를 시각적으로 표현하는 방법**이다.

스펙트로그램을 계산하는 과정에는 눈에 보이지 않는 몇 가지 추가적인 처리들이 포함되어 있다.  
먼저, **y축(주파수 축)은 로그 스케일(log scale)** 로 변환된다.  
또한, **색상으로 표현되는 크기 값은 데시벨(dB)** 로 변환되는데,  
이는 **진폭을 로그 스케일로 표현한 것**이라고 생각하면 된다.

이러한 변환이 적용되는 이유는,  
인간이 인지할 수 있는 주파수와 진폭의 범위가  
전체 범위 중에서도 **아주 좁고 제한적인 구간에 집중되어 있기 때문**이다.


```python
spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)
mel_spect = librosa.power_to_db(spect, ref=np.max)
librosa.display.specshow(mel_spect, y_axis='mel', fmax=8000, x_axis='time');
plt.title('Mel Spectrogram');
plt.colorbar(format='%+2.0f dB');
```
![System Overview](/assets/images/Yolopose/Mel_Spectrogram_4.png)  
*그림 9: 멜 필터 뱅크를 적용하여 계산한 오디오 신호의 멜 스펙트로그램*

---

## 6. Blob Detection
![System Overview](/assets/images/Yolopose/blob1.png)  
*그림 10: 아래에서 설명하는 기능들을 시연하기 위해 사용한 이미지*

컴퓨터 비전에서 **주변 환경과 구별되는 블롭(blob, 영역)** 을 검출하는 것은  
매우 일반적이면서도 강력한 기법이다.  
블롭은 이미지 속의 **작은 밝은 점**처럼 단순할 수도 있고,  
비디오 속에서 **움직이는 객체**처럼 복잡한 형태일 수도 있다.

블롭 검출(blob detection)은 현미경 영상 분석, 감시 시스템, 객체 추적,  
천문학, 의료 영상 등 다양한 분야에서 핵심적인 역할을 한다.

이 글에서는 블롭 검출의 **이론적 개념과 수학적 기반**을 이해하고,  
Python과 C++에서 OpenCV의 **SimpleBlobDetector**를 사용하여  
블롭 검출을 구현해 본다.  
또한, 파라미터 튜닝 방법과 함께 **컨투어(contours)** 및  
**연결 성분(connected components)** 과 같은 대안적인 접근 방식도 살펴본다.

### 6.1 Blob이란 무엇인가?

블롭(blob)은 이미지 내에서 **밝기(intensity)나 색상(color)** 과 같은 특정 속성이  
**일정하거나 정의된 범위 내에서 변화하는 영역(region)** 을 의미한다.  
블롭은 일반적으로 시각적으로 구별 가능한 구조를 가지며, 다음과 같은 형태로 나타난다.

- 밝거나 어두운 점(spot)
- 균일한 색상 또는 텍스처를 가진 영역
- 대략적으로 원형 또는 타원형 모양의 객체

보다 단순하게 말하면,  
블롭은 **하나의 정보 단위로 간주되는 연결된 픽셀들의 집합**이다.

### 6.2 Blob 검출은 어떻게 동작하는가?

이름에서 알 수 있듯이, **SimpleBlobDetector**는 비교적 단순한 알고리즘을 기반으로 한다.  
이 알고리즘은 여러 개의 **파라미터(아래에서 굵게 표시됨)** 에 의해 제어되며,  
다음과 같은 단계로 구성된다.

### 6.3 임계값 처리
소스 이미지를 여러 개의 **이진 이미지(binary image)** 로 변환한다.  
이를 위해 `minThreshold`부터 시작하여 `maxThreshold`까지  
`thresholdStep`만큼 증가시키며 임계값 처리를 수행한다.

즉,
- 첫 번째 임계값: `minThreshold`
- 두 번째 임계값: `minThreshold + thresholdStep`
- 세 번째 임계값: `minThreshold + 2 × thresholdStep`
- …

이 과정을 통해 서로 다른 임계값을 적용한 다수의 이진 이미지를 생성한다.


### 6.4 그룹화
각 이진 이미지에서 **서로 연결된 흰색 픽셀들**을 하나의 그룹으로 묶는다.  
이렇게 생성된 각 그룹을 **이진 블롭(binary blob)** 이라고 부른다.

### 6.5 병합
이진 이미지들에서 검출된 블롭들의 **중심점(center)** 을 계산한 뒤,  
서로의 거리가 `minDistBetweenBlobs`보다 작은 블롭들은  
하나의 블롭으로 병합한다.


### 6.6 중심 및 반지름 계산
최종적으로 병합된 블롭들에 대해  
**중심점과 반지름(radius)** 을 계산하여 결과로 반환한다.


### 6.7 필터링 블롭
![System Overview](/assets/images/Yolopose/blob2.png)  
*그림 11: 블롭 필터(Blob Filters)*

SimpleBlobDetector의 파라미터들은  
우리가 **어떤 유형의 블롭(blob)을 검출할지 선택적으로 필터링**할 수 있도록 설정할 수 있다.

### 🔹 색상 기준 필터링 (By Color)
먼저 `filterByColor = 1`로 설정해야 한다.

- `blobColor = 0` : 어두운(blacks) 블롭 선택
- `blobColor = 255` : 밝은(whites) 블롭 선택

### 🔹 크기 기준 필터링 (By Size)
블롭의 크기를 기준으로 필터링하려면  
`filterByArea = 1`로 설정한 뒤, `minArea`와 `maxArea` 값을 지정한다.

예를 들어,
- `minArea = 100`으로 설정하면  
  **픽셀 수가 100 미만인 블롭들은 모두 제거**된다.

### 🔹 형태 기준 필터링 (By Shape)
형태 기반 필터링은 세 가지 서로 다른 파라미터로 구성된다.


#### 원형도 (Circularity)
원형도는 블롭이 **얼마나 원(circle)에 가까운지**를 측정한다.  
예를 들어, **정육각형**은 **정사각형**보다 더 높은 원형도를 가진다.

- 원형도로 필터링하려면 `filterByCircularity = 1`로 설정
- 이후 `minCircularity`와 `maxCircularity` 값을 지정

일반적으로:
- 완전한 원(circle)의 원형도는 **1**
- 정사각형(square)의 원형도는 **약 0.785**


#### 볼록도 (Convexity)
볼록도는 다음과 같이 정의된다.

> **Convexity = (블롭의 면적) / (블롭의 볼록 껍질(convex hull) 면적)**

여기서 **볼록 껍질(convex hull)** 이란,  
해당 도형을 완전히 감싸는 가장 작은 볼록 다각형을 의미한다.

- 볼록도로 필터링하려면 `filterByConvexity = 1`로 설정
- `0 ≤ minConvexity ≤ 1`
- `maxConvexity ≤ 1`


#### 관성 비율 (Inertia Ratio)
이름 때문에 어렵게 느껴질 수 있지만,  
실제로는 **도형이 얼마나 길쭉한지(elongated)** 를 나타내는 지표이다.

일반적인 예시는 다음과 같다.
- 원(circle): **1**
- 타원(ellipse): **0과 1 사이**
- 직선(line): **0**

- 관성 비율로 필터링하려면 `filterByInertia = 1`로 설정
- `0 ≤ minInertiaRatio ≤ 1`
- `maxInertiaRatio ≤ 1`

### 6.8 블롭 검출 기법

### 🔹 가우시안 라플라시안 (Laplacian of Gaussian, LoG)

**가우시안 라플라시안(LoG)** 은 고전적인 블롭 검출 기법이다.  
이 방법에서는 먼저 **가우시안 커널(Gaussian kernel)** 을 사용해 이미지를 부드럽게 만든 뒤,  
**라플라시안 연산자(Laplacian operator, 2차 미분)** 를 적용하여 필터링한다.

LoG는 다음과 같이 정의된다.

$$
\text{LoG}(x, y, \sigma)
=
\sigma^2 \cdot \nabla^2 \big( G(x, y, \sigma) * I(x, y) \big)
$$


각 항의 의미는 다음과 같다.

- **G(x, y, σ)** : 표준편차 σ를 갖는 가우시안 함수
- **∇²** : 라플라시안 연산자
- **I(x, y)** : 입력 이미지


LoG 기법은 서로 다른 크기의 블롭들을 효과적으로 검출할 수 있다는 장점이 있다.  
하지만 하나의 블롭에 대해 **여러 개의 강한 응답(response)** 이 발생할 수 있다는 단점도 있다.

이러한 중복 검출을 제거하기 위해,  
일반적으로 **비최대 억제(non-maximum suppression)** 와 같은 후처리 기법을 적용하여  
각 블롭에 대해 가장 강한 응답만 남기고 나머지는 제거한다.

![System Overview](/assets/images/Yolopose/blob3.png)  
*그림 12: LoG 기법 output*

### 🔹 가우시안 차분 (Difference of Gaussian, DoG)

**Difference of Gaussian(DoG)** 는  
**가우시안 라플라시안(LoG)을 근사한 방법**으로,  
계산 효율이 더 높은 블롭 검출 기법이다.

DoG 알고리즘은 **서로 다른 표준편차를 가진 두 개의 가우시안 필터로 평활화된 이미지 간의 차이**를 계산한다.  
이 방법은 **특정 크기 범위의 블롭(blob)을 검출하는 데 매우 유용**하다.

가우시안 커널의 **표준편차(σ)** 를 조절함으로써  
검출되는 블롭의 **크기(scale)** 를 제어할 수 있다.

DoG는 다음과 같이 정의된다.

$$
\text{DoG}(x, y)
=
G(x, y, k\sigma)
-
G(x, y, \sigma)
$$


- **k** : 스케일 배율 인자(scale multiplication factor)
- **G(x, y, σ)** : 표준편차 σ를 갖는 가우시안 함수

![System Overview](/assets/images/Yolopose/blob4.png)  
*그림 13: DOG 기법 output*

### 🔹 헤시안 행렬식 (Determinant of Hessian, DoH)

**Determinant of Hessian(DoH)** 기법은  
**2차 미분(second-order derivatives)** 에 기반하여  
이미지에서 **강한 곡률 변화(curvature change)** 가 발생하는 지점을 찾는 데 초점을 둔다.

DoH 접근법에서는 **헤시안 행렬(Hessian matrix)의 행렬식(determinant)** 을 사용하여  
블롭(blob)을 식별한다.  
이미지의 **국소적인 곡률(local curvature)** 은 헤시안 행렬로 표현되며,  
이 헤시안 행렬식의 **국소 최대값(local maxima)** 을 통해  
서로 다른 크기와 형태를 가진 블롭들을 찾을 수 있다.

헤시안 행렬식은 다음과 같이 정의된다.

$$
\text{Det}(H)
=
I_{xx} I_{yy}
-
I_{xy}^{2}
$$

여기서,

$$
I_{xx} = \frac{\partial^2 I}{\partial x^2}, \quad
I_{yy} = \frac{\partial^2 I}{\partial y^2}, \quad
I_{xy} = \frac{\partial^2 I}{\partial x \partial y}
$$


![System Overview](/assets/images/Yolopose/blob5.png)  
*그림 14: DOH 기법 output*


![System Overview](/assets/images/Yolopose/blob6.png)  
*그림 15: 프로젝트 최종 output1*

![System Overview](/assets/images/Yolopose/blob7.png)  
*그림 16: 프로젝트 최종 output2*


---

## 7. 기술구현 – 데이터셋 학습 결과

본 장에서는 데이터셋 규모 확장에 따른 YOLOv8 객체 검출 모델의 학습 결과를 정리하고,  
최종 모델의 성능을 정량적·정성적으로 분석한다.

### 7.1 소규모 데이터셋 학습 결과 (Class별 100장)
![System Overview](/assets/images/Yolopose/Confidence1.png)  
*그림 17: Class 별 100장*
초기 실험에서는 클래스별 약 100장의 이미지를 사용하여 모델을 학습하였다.  
F1–Confidence Curve 분석 결과, 일부 클래스에서는 비교적 안정적인 성능을 보였으나  
전반적으로 confidence threshold 변화에 따라 성능 편차가 크게 나타났다.

- 전체 클래스 기준 최고 F1 점수: **0.94**
- 최적 confidence threshold: **약 0.624**
- 데이터 수 부족으로 인해 특정 클래스에서 과검출 및 미검출 현상 발생

이 단계에서는 모델이 객체의 전반적인 특징을 학습하기에는 데이터가 충분하지 않음을 확인하였다.


### 7.2 중간 규모 데이터셋 학습 결과 (Class별 200~300장)
![System Overview](/assets/images/Yolopose/Confidence2.png)  
*그림 18: Class 별 2_300장*
데이터 수를 클래스별 200~300장으로 확장한 후 재학습을 수행하였다.  
이 단계에서는 F1–Confidence Curve가 전반적으로 평탄해지며,  
confidence threshold 변화에 대한 민감도가 감소하였다.

- 전체 클래스 기준 최고 F1 점수: **0.98**
- 최적 confidence threshold: **약 0.647**
- 대부분의 클래스에서 안정적인 검출 성능 확보
- 소형 객체 및 가려진 객체에 대한 검출 성능 개선

데이터 수 증가가 모델의 일반화 성능 향상에 직접적인 영향을 미침을 확인할 수 있었다.

### 7.3 대규모 데이터셋 학습 결과 (누적 약 4,000장)
![System Overview](/assets/images/Yolopose/Confidence3.png)  
*그림 19: 누적 4,000장*
최종적으로 클래스별 300~400장, 전체 약 4,000장의 데이터셋을 구성하여 학습을 진행하였다.  
이 단계에서는 F1 Curve뿐만 아니라 Precision–Confidence Curve 또한 함께 분석하였다.

- 전체 클래스 기준 최고 F1 점수: **0.98**
- 최적 confidence threshold: **0.662**
- Precision–Confidence Curve에서 고신뢰도 영역에서 **Precision ≈ 1.0** 유지
- 높은 신뢰도 임계값에서도 오탐(false positive)이 거의 발생하지 않음

이를 통해 실제 환경에서 confidence threshold 조절을 통해  
정밀도와 재현율을 상황에 맞게 제어할 수 있음을 확인하였다.

### 7.4 최종 성능 평가 및 혼동 행렬 분석

최종 모델에 대해 정규화된 혼동 행렬(Confusion Matrix)을 기반으로 성능을 분석하였다.

- **mAP@0.5**: **0.991**
- **최대 재현율(Recall)**: **1.00**  
  → confidence threshold = 0.0일 때 모든 실제 객체를 검출
- **최대 정밀도(Precision)**: **1.00**  
  → confidence threshold = 0.976일 때 예측된 모든 객체가 정답

대부분의 클래스에서 매우 높은 분류 정확도를 보였으며,  
특히 `book`, `wallet`, `laptop`, `vitamin`, `airpods`, `pencilcase` 클래스는  
거의 완벽한 분류 성능을 달성하였다.

다만, 배경(background) 클래스의 경우  
실제 배경을 `phone`(약 70%), `hand`(약 20%)로 오인식하는 경향이 관찰되었으며,  
이는 향후 배경 데이터 확장 또는 negative sample 보강을 통해 개선이 필요하다.

### 7.5 종합 분석
![System Overview](/assets/images/Yolopose/Confusion_Matrix_Normalized.png)  
*그림 20: Confusion_Matrix_Normalized*
- 데이터 수 증가에 따라 모델의 **안정성 및 일반화 성능이 크게 향상**
- F1, Precision, Recall, mAP 전반에서 **실사용 가능한 수준의 성능 확보**
- 최적 confidence threshold(≈0.66)에서 정밀도–재현율 균형이 가장 우수
- 향후 과제로는 배경 클래스 오탐 감소 및 클래스 간 경계 강화가 필요

본 결과를 통해, 충분한 데이터셋 구성과 단계적 학습이  
객체 검출 모델 성능에 결정적인 영향을 미친다는 것을 확인하였다.

---

## 8. 문제점 및 해결 방안

본 프로젝트를 수행하는 과정에서 기술적·구조적인 여러 문제점이 도출되었으며,  
이에 대한 원인 분석과 해결 방안을 다음과 같이 정리하였다.

### 8.1 문제점

- **유사한 색상 및 형태의 객체에 대한 YOLO 검출 오류**
  - 배경과 객체의 색상·질감이 유사할 경우 오탐 발생
  - 특히 background가 `phone`, `hand`로 오인식되는 경우 다수 관찰

- **힘 제어(control) 연계 과정에서의 난이도**
  - 비전 기반 인식 결과를 실시간 제어 로직과 안정적으로 결합하는 데 한계 존재

- **한국어 wake-up word 기반 STT 모델(.tflite) 생성 실패**
  - 데이터 수 부족 및 학습 안정성 문제로 인해 경량 모델 생성에 실패

---

### 8.2 해결 방안

- **Negative sample 및 배경 데이터 강화**
  - 배경만 포함된 이미지 및 유사 색상 객체 데이터 추가 수집
  - confidence threshold 조절을 통한 오탐 감소

- **제어 파트 분리 및 단계적 통합**
  - 비전 인식과 제어 알고리즘을 모듈 단위로 분리하여 안정성 확보
  - 인식 신뢰도 기반 제어 조건 추가

- **STT 파이프라인 재설계**
  - 한국어 wake-up word를 cloud 기반 STT 또는 경량 keyword spotting 모델로 대체 고려
  - 데이터 증강 및 다국어 pretrained 모델 활용 방안 검토

---

## 9. 성능 평가 및 결과

최종 모델은 데이터셋 확장 및 반복 학습을 통해  
객체 검출 성능과 안정성 측면에서 유의미한 성과를 달성하였다.

- **최적 F1 Score**: **0.98**
- **최적 Confidence Threshold**: **0.662**
- **mAP@0.5**: **0.991**
- **최대 Recall**: **1.00**
- **최대 Precision**: **1.00**

혼동 행렬 분석 결과, 대부분의 클래스에서 매우 높은 분류 정확도를 보였으며  
실제 환경에서도 안정적인 객체 검출이 가능함을 확인하였다.

특히 데이터 수 증가에 따라  
- F1–Confidence Curve의 변동성이 감소
- Precision–Recall 균형이 개선
되는 경향이 명확하게 나타났다.

---

## 10. 향후 연구 및 발전 방향

본 프로젝트를 기반으로 다음과 같은 확장 및 고도화가 가능하다.

- **다양한 태스크 추가를 통한 상업적 활용성 강화**
  - 객체 인식 기반 서비스 시나리오 확장
  - 멀티 태스크 비전 모델로 확장 가능성

- **STT 안정화 및 서비스 수준 개선**
  - 실시간 음성 인식 안정성 확보
  - 음성–비전 멀티모달 인터페이스 구축

- **Pose Estimation 경량화**
  - Keypoint 수를 17개 → 4개로 축소
  - 알고리즘 계산량 감소를 통한 실시간 성능 향상

- **Edge Device 최적화**
  - 경량 모델(TFLite, ONNX) 변환 및 추론 최적화
  - 저사양 환경에서의 실사용 가능성 검증

---

## 11. 결론

본 프로젝트에서는 객체 검출, 음성 인식, 제어 로직을 포함한  
비전 기반 시스템을 설계하고 구현하였다.

데이터셋 구성부터 모델 학습, 성능 평가에 이르는 전 과정을 통해  
다음과 같은 성과를 얻을 수 있었다.

- 컴퓨터 비전 전반에 대한 **구조적 이해**
- YOLO 기반 객체 검출 모델의 **학습 및 튜닝 경험**
- STT, TTS 시스템의 **구조 및 구현 방식에 대한 이해**
- 실제 환경에서 발생하는 문제를 분석하고 개선하는 **엔지니어링 역량 향상**

본 프로젝트는 단순한 모델 구현을 넘어,  
**실제 적용 가능한 시스템을 설계하고 평가했다는 점에서 의미 있는 결과**를 가진다.  
향후 연구 및 개선을 통해 보다 안정적이고 확장 가능한 지능형 시스템으로 발전시킬 수 있을 것이다.


---

## Acknowledgments

---

## References

1. [Yolov11n](https://docs.ultralytics.com/ko/models/yolo11/#overview)

2. [Mel Spectrogram](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53)

3. [Blob Detection](https://opencv.org/blog/blob-detection-using-opencv/)




---

## Appendix A: Technical Specifications

$$
\text{LoG}(x, y, \sigma)
=
\sigma^2 \cdot \nabla^2 \big( G(x, y, \sigma) * I(x, y) \big)
$$

$$
\text{DoG}(x, y)
=
G(x, y, k\sigma)
-
G(x, y, \sigma)
$$

$$
\text{Det}(H)
=
I_{xx} I_{yy}
-
I_{xy}^{2}
$$


## Appendix B: Hardware Specifications

#### B.1 Doosan M0609 Robot Specifications

| Parameter | Value | Unit |
|-----------|-------|------|
| Degrees of Freedom | 6 | - |
| Reach | 900 | mm |
| Payload | 6 | kg |
| Repeatability | ±0.05 | mm |
| Joint Speed (Max) | 180-360 | °/s |
| Operating Temperature | 0-45 | °C |
| Power Consumption | 1.2 | kW |

#### B.2 Intel RealSense D435i Camera Specifications

| Parameter | Value | Unit |
|-----------|-------|------|
| Product Collection | Intel® RealSense™ Cameras | - |
| Code Name | Double Springs | - |
| Launch Date | Q1 2019 | - |
| Depth Technology | Active Stereoscopic | - |
| Operating Range | ~0.3 – 3.0 | m |
| Depth Resolution | 1280 × 720 | pixels |
| Depth Frame Rate | 30 | fps |
| Depth Field of View (H × V) | 87 × 58 | degrees |
| RGB Sensor | Yes | - |
| IMU (Inertial Measurement Unit) | Yes | - |
| Tracking Module | No | - |
| Dimensions | 90 × 25 × 25 | mm |
| System Interface | USB 3.0 | - |
